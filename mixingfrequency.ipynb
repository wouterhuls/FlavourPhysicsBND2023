{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wouterhuls/FlavourPhysicsBND2023/blob/main/mixingfrequency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-k9pmBVBqep"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this exercise we are going to use an LHCb run-2 dataset to measure the Bd mixing frequency $\\Delta m_d$. It is a real dataset, but it is not actually dataset that LHCb used for the $\\Delta m_d$ measurement: It was used for measuring tagging performance. Yet, the result will be reaonable competitive with the world average.\n",
        "\n",
        "The learning goals of this exercise are:\n",
        "* fitting with zfit, a python based modeling package based on Tensorflow. For more instructions, visit the zfit documentation.\n",
        "* making s-plots and using s-weights for fitting\n",
        "* plotting an asymmetry and measuring an oscillation\n",
        "\n",
        "The analysis will consist of two steps. In the first step you will learn about zfit and s-weights. In the second step you will measure the mixing frequency.\n",
        "\n",
        "# The physics\n",
        "\n",
        "We will perform the measurement using a sample of $B_d \\to J/\\psi K^{*0}$ event, with $J/\\psi\\to\\mu^+\\mu^-$ and $K^{*0}\\to K^+ \\pi^-$. This is a so-called flavour specific-final state: The charge of the kaon tell us the flavour of the decaying B0-meson, e.g. whether it was decaying as a $B^0$ or $\\bar{B}^0$.\n",
        "\n",
        "To measure the time-dependent oscillations we need two ingredients, namely:\n",
        "* the decay time\n",
        "* the flavour of the B at production\n",
        "You have learned in the lectures that the the rate for a B produced as $B^0$ to decay as $B^0$ is given by\n",
        "\n",
        "$ N( B^0 \\to B^0 ) = \\frac{e^{-t/\\tau}}{2} ( 1 + \\cos( \\Delta m t) ) $\n",
        "\n",
        "$ N( B^0 \\to \\bar{B}^0 ) = \\frac{e^{-t/\\tau}}{2} ( 1 - \\cos( \\Delta m t) ) $\n",
        "\n",
        "The formulas for B-mesons starting their life as an $\\bar{B}^0$ can be obtained by swapping $B^0$ and $\\bar{B}^0$.\n",
        "\n",
        "There are two important experimental effects for this measurement:\n",
        "* the sample has a non-negligible background\n",
        "* the flavour tagging has a considerable 'mis-tag rate'\n",
        "\n",
        "In the following we will have to deal with these two effects.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7tOUWwN8k-A2",
        "outputId": "dbd5c53f-faa6-4bff-c5d2-83affda7dd20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!conda install uproot --channel conda-forge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOWwkK8AF-sh"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Look up in the PDG (google `pdg live`) the quark content of these mesons: $B^0$, $J/\\psi$, $K^{*0}$. Draw the Feynman diagram for the decay $B^0 \\to J/\\psi \\bar{K}^{*0}$ (on a piece of paper, or on your tablet.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4QC0p0Bc3VH"
      },
      "source": [
        "# Prerequites\n",
        "\n",
        "Install the `zfit` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arsl6VsJfEbB"
      },
      "outputs": [],
      "source": [
        "# @ Prerequisites\n",
        "import platform\n",
        "print(platform.python_version())\n",
        "\n",
        "#!conda install uproot --channel conda-forge\n",
        "#!conda install zfit --channel conda-forge\n",
        "#!conda install hepstats --channel conda-forge\n",
        "#!conda install mlphep --channel conda-forge\n",
        "# In google colab, use pip rather than conda\n",
        "!pip install zfit\n",
        "!pip install hepstats\n",
        "!pip install mplhep\n",
        "!pip install uproot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_T7oPHje3py"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "Attached to this workbook you will find a 'ROOT' file. The file contains a TTree (named `tree`) with a number of fields (called `branches` in ROOT language). For the rest of the exercise, the relevant fields are:\n",
        "* `mass`: the B candidate invariant mass in MeV\n",
        "* `decaytime`: the B candidate decaytime in ns\n",
        "* `q`: the charge of the B candidate reconstructed by the flavour tagging algorithm\n",
        "* `eta`: the mistagrate assigned by the flavour tagging algorithm\n",
        "* `pid`: the PDG value that the LHCb software assigned to the decaying B: This can be either 512 (for $B^0$) or -512 (for anti-$B^0$), depending on whether the kaon was $K^+$ or $K^-$.\n",
        "\n",
        "Load the dataset with your favourite tool and draw the reconstructed invariant mass. If you plot it on a log scale, you will find one peak on a falling exponential background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p_LmdAWk-A5"
      },
      "outputs": [],
      "source": [
        "! pip install uproot\n",
        "import uproot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK2PJdqUfiS_"
      },
      "outputs": [],
      "source": [
        "#@title Example solution\n",
        "# This is a partial solution to the exercise using uproot. You can also use pyroot if you prefer.\n",
        "url = 'http://www.nikhef.nl/~wouterh/tmp/kstarntuple_for_BND.root'\n",
        "\n",
        "import uproot\n",
        "events = uproot.open(url + \":tree\")\n",
        "mass = events[\"mass\"].array()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(mass, bins=200)\n",
        "plt.show()\n",
        "\n",
        "# Hint: you will see a lot more if you plot on a log scale and with more bins!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EcQ5XaTWdDi"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Draw also the B candidate 'decaytime'. The units are in nanoseconds. Compute the average decaytime and its statistical error. How does the answer compare to the average $B^0$ lifetime in the PDG? Give two reasons why the two are different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOtdtg19sNZJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3KP9Vw7bUfm"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "We will now perform a fit to the invariant mass distribution to extract the number of $B^0$ events. Because it may take you too much time to figure this out yourself, we have written most of the code for you.\n",
        "\n",
        "If you look at the final fit result superimposed on the data set, it looks pretty bad. One reason is the 'signal mass model': it is not very well described by a Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51V7MhV_fnfe"
      },
      "outputs": [],
      "source": [
        "import zfit\n",
        "import numpy as np\n",
        "\n",
        "# Specify the mass range. To simplify the fit, we first limit the mass range to the region just around the B0 mass peak.\n",
        "massmin = 5150\n",
        "massmax = 5350\n",
        "\n",
        "# temporary hack, to make sure we can rerun this cell as often as we like.\n",
        "from collections import OrderedDict\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "# use uproot to get the subset of events in this mass range in numpy format\n",
        "npevents = events.arrays( library='np',expressions=['mass','decaytime','masserr'] )\n",
        "mass = npevents[ 'mass' ]\n",
        "masserr = npevents[ 'masserr' ]\n",
        "decaytime = npevents['decaytime']\n",
        "\n",
        "# use a little bit complicated logic to get access to the events in the narrow\n",
        "# mass range in such a way that we can still identify the events in the\n",
        "# original list. (we need that later)\n",
        "mask = np.logical_and(mass>massmin,mass<massmax,masserr<10)\n",
        "indices = np.where(mask)\n",
        "\n",
        "# create a zfit data set from the numpy array. when constructing a zfit dataset\n",
        "# from a numpy array we need to tell how we 'name' the columns\n",
        "massobs = zfit.Space(\"mass\",(massmin,massmax))\n",
        "zdata = zfit.Data.from_numpy( array = mass[indices], obs = massobs )\n",
        "\n",
        "# create a zfit pdf for the B0 signal\n",
        "mu_B0 = zfit.Parameter(\"mu_B0\", 5279, 5250, 5300)\n",
        "sigma_B0 = zfit.Parameter(\"sigma_B0\", 10, 0, 30)\n",
        "masspdf_B0 = zfit.pdf.Gauss(mu=mu_B0, sigma=sigma_B0, obs=massobs)\n",
        "\n",
        "# create a zfit pdf for the exponential background\n",
        "lambd = zfit.Parameter(\"lambda\", -0.001, -1,+1)\n",
        "masspdf_bkg = zfit.pdf.Exponential(lambd, obs=massobs)\n",
        "\n",
        "# create an extended PDF from the sum of these\n",
        "nev = len( mass )\n",
        "yield_B0  = zfit.Parameter(\"yield_B0\", 0.9*nev, -0.1*nev, 1.1*nev)\n",
        "yield_bkg = zfit.Parameter(\"yield_bkg\", 0.1*nev, -0.1*nev, 1.1*nev)\n",
        "extmasspdf_B0  = masspdf_B0.create_extended(yield_ = yield_B0)\n",
        "extmasspdf_bkg = masspdf_bkg.create_extended(yield_ = yield_bkg)\n",
        "pdf_total  = zfit.pdf.SumPDF([extmasspdf_B0, extmasspdf_bkg], name=\"totPDF\")\n",
        "\n",
        "# create a loss function. this is what we will 'minimize'\n",
        "nll_data = zfit.loss.ExtendedUnbinnedNLL(model=pdf_total, data=zdata)\n",
        "# create the minimizer. This one uses minuit, but there are various alternatives.\n",
        "minimizer = zfit.minimize.Minuit()\n",
        "result = minimizer.minimize(nll_data)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n",
        "# draw the result\n",
        "n_bins = 200\n",
        "npdata = zdata['mass'].numpy()\n",
        "plot_scaling = len(npdata) / n_bins * massobs.area()\n",
        "x = np.linspace(massmin,massmax, 1000)\n",
        "y = pdf_total.pdf(x).numpy()\n",
        "fig, axes = plt.subplots(2)\n",
        "axes[1].set_yscale(\"log\")\n",
        "for i in range(2):\n",
        "  axis = axes[i]\n",
        "  color = 'black'\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"step\")\n",
        "  axis.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=2)\n",
        "  axis.set_xlabel(\"mass [MeV]\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMdmAGB1pPSX"
      },
      "outputs": [],
      "source": [
        "# repeat the fit but with a better mass model\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "aL = zfit.Parameter(\"aL_B0\",  1.4, 0.1, 5,floating=True)\n",
        "aR = zfit.Parameter(\"aR_B0\",  1.4, 0.1, 5,floating=True)\n",
        "aR = aL\n",
        "nL = zfit.Parameter(\"nL_B0\", 6, 1., 10, floating=True)\n",
        "nR = zfit.Parameter(\"nR_B0\", 10, 1., 20,floating=True)\n",
        "\n",
        "masspdf_B0 = zfit.pdf.DoubleCB(obs=massobs, mu=mu_B0, sigma=sigma_B0, alphal=aL, nl=nL, alphar=aR, nr=nR)\n",
        "\n",
        "extmasspdf_B0  = masspdf_B0.create_extended(yield_ = yield_B0)\n",
        "extmasspdf_bkg = masspdf_bkg.create_extended(yield_ = yield_bkg)\n",
        "pdf_total  = zfit.pdf.SumPDF([extmasspdf_B0, extmasspdf_bkg], name=\"totPDF\")\n",
        "\n",
        "nll_data = zfit.loss.ExtendedUnbinnedNLL(model=pdf_total, data=zdata)\n",
        "# create the minimizer. This one uses minuit, but there are various alternatives.\n",
        "result = minimizer.minimize(nll_data)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n",
        "# draw the result\n",
        "n_bins = 200\n",
        "npdata = zdata['mass'].numpy()\n",
        "plot_scaling = len(npdata) / n_bins * massobs.area()\n",
        "x = np.linspace(massmin,massmax, 1000)\n",
        "y = pdf_total.pdf(x).numpy()\n",
        "fig, axes = plt.subplots(2)\n",
        "axes[1].set_yscale(\"log\")\n",
        "for i in range(2):\n",
        "  axis = axes[i]\n",
        "  color = 'black'\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"step\")\n",
        "  axis.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=2)\n",
        "  axis.set_xlabel(\"mass [MeV]\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPrbQlea2QRQ"
      },
      "outputs": [],
      "source": [
        "# @ compute s-weights and make a decaytime plot\n",
        "\n",
        "# we can get rid of the masks if we only work with the selected events. that actually makes a bit more sense.\n",
        "from hepstats.splot import compute_sweights\n",
        "sweights_all = compute_sweights(pdf_total, npdata)\n",
        "sweights_B0 = np.zeros_like( mass, dtype=np.float64 )\n",
        "np.place(sweights_B0, mask, sweights_all[yield_B0] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW_KeeQw6J7B"
      },
      "outputs": [],
      "source": [
        "# plot the s-weighted decay time distribution\n",
        "import matplotlib.pyplot as plt\n",
        "decaytime = npevents['decaytime']\n",
        "q = events[\"q\"].array()\n",
        "eta = events[\"eta\"].array()\n",
        "plt.hist(decaytime, bins=200, weights = sweights_B0 * (pid*q<0)* (1-2*eta))\n",
        "plt.hist(decaytime, bins=200, weights = sweights_B0 * (pid*q>0)* (1-2*eta))\n",
        "plt.show()\n",
        "\n",
        "# suggested binning for asymmetry plot\n",
        "#tbins = np.concatenate((np.arange(0.0000,5.0,step=0.5),[5.6,6.2,7.0,7.8,8.7,9.7,10.8,12.0,13.4,15.]))\n",
        "tbins = np.array([0.002,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.2,6.2,7.5,8.8,10.2,11.5,15.])\n",
        "tbins = tbins/1000.\n",
        "\n",
        "# choose 20 bins with equal number of events\n",
        "#tbins = np.quantile(decaytime, np.linspace(start=0.0,stop=1.0,num=21)[1:])\n",
        "#tbins[-1] = 0.015\n",
        "#tbins[0]  = 0.0002\n",
        "\n",
        "# I'm not sure how to do this properly\n",
        "qD = q*(1-2*eta)*-1\n",
        "wqDsum, bin_edges  = np.histogram(decaytime,bins=tbins,weights=sweights_B0*qD)\n",
        "wqD2sum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*qD*qD)\n",
        "w2qD2sum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*sweights_B0*qD*qD)\n",
        "asymmetry    = wqDsum / wqD2sum\n",
        "asymmetryerr = np.sqrt(w2qD2sum) / wqD2sum\n",
        "\n",
        "# compute in every bin the average decay time\n",
        "wtsum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*decaytime)\n",
        "wsum,  bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0)\n",
        "avtime = wtsum / wsum\n",
        "\n",
        "# now draw points with both vertical and horizontal errors\n",
        "xerrors = [avtime-bin_edges[:-1],bin_edges[1:]-avtime]\n",
        "plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6r1fKQlfcGQ"
      },
      "source": [
        "# compute s-weights corresponding to each of the yields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjK2Q54aIfAd"
      },
      "outputs": [],
      "source": [
        "# define a model and fit to s-weighted data\n",
        "\n",
        "# for now, just measure the amplitude of the sin-wave\n",
        "# this only works because C=0\n",
        "deltaM = 507. # mixing frequency in [2pi/ns]\n",
        "qDsin = qD * np.sin(decaytime*deltaM)\n",
        "wqDsinsum = np.sum( sweights_B0*qDsin )\n",
        "wqDsin2sum = np.sum( sweights_B0*np.square(qDsin) )\n",
        "w2qDsin2sum = np.sum( np.square(sweights_B0*qDsin) )\n",
        "\n",
        "Sasymmetry    = wqDsinsum / wqDsin2sum\n",
        "Sasymmetryerr = np.sqrt(w2qDsin2sum) / wqDsin2sum\n",
        "print(\"S = %5.3f +/- %5.3f\" %(Sasymmetry,Sasymmetryerr))\n",
        "\n",
        "# the following code extracts S and C simultanously\n",
        "qDcos = qD * np.cos(decaytime*deltaM)\n",
        "wqDcossum = np.sum( sweights_B0*qDcos )\n",
        "wqDcos2sum = np.sum( sweights_B0*np.square(qDcos) )\n",
        "w2qDcos2sum = np.sum( np.square(sweights_B0*qDcos) )\n",
        "wqD2cossinsum = np.sum( sweights_B0*qDcos*qDsin )\n",
        "w2qD2cossinsum = np.sum( np.square(sweights_B0)*qDcos*qDsin )\n",
        "\n",
        "b = np.array( [ wqDsinsum,wqDcossum] )\n",
        "\n",
        "A = np.array( [[wqDsin2sum,wqD2cossinsum],[wqD2cossinsum,wqDcos2sum]] )\n",
        "x = np.linalg.solve(A,b)\n",
        "print(x/0.803)\n",
        "S = x[0]\n",
        "C = x[1]\n",
        "\n",
        "# the only thing to do is to get the weights squared correction\n",
        "wqD2cossinsum = np.sum( sweights_B0*qDcos*qDsin )\n",
        "B = np.array( [[w2qDsin2sum,w2qD2cossinsum],[w2qD2cossinsum,w2qDcos2sum]] )\n",
        "Ainv = np.linalg.inv(A)\n",
        "cov = Ainv @ B @ Ainv\n",
        "\n",
        "print(\"S = %5.3f +/- %5.3f\" %(S,np.sqrt(cov[0,0])))\n",
        "print(\"C = %5.3f +/- %5.3f\" %(C,np.sqrt(cov[1,1])))\n",
        "\n",
        "\n",
        "# create a function to superimpose on the asymmetry\n",
        "x = np.linspace(0,0.015, 1000)\n",
        "y = S*np.sin(x*deltaM) + C*np.cos(x*deltaM)\n",
        "plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8 (Wouter)",
      "language": "python",
      "name": "shared_python38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}