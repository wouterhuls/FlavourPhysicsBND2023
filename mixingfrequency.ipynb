{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wouterhuls/FlavourPhysicsBND2023/blob/main/mixingfrequency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-k9pmBVBqep"
      },
      "source": [
        "# Measuring the $B_d$ mixing frequency with $B_d \\to J/\\psi K^{*0}$ events  \n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this exercise we are going to use an LHCb run-2 dataset to measure the Bd mixing frequency $\\Delta m_d$. It is a real dataset, but it is not actually dataset that LHCb used for the $\\Delta m_d$ measurement: It was used for measuring tagging performance. Yet, the result will be reasonable competitive with the world average.\n",
        "\n",
        "The learning goals of this exercise are:\n",
        "* fitting with zfit, a python based modeling package based on Tensorflow. For more instructions, visit the zfit documentation.\n",
        "* making s-plots and using s-weights for fitting\n",
        "* plotting an asymmetry and measuring an oscillation\n",
        "\n",
        "## The physics\n",
        "\n",
        "We will perform the measurement using a sample of $B_d \\to J/\\psi K^{*0}$ event, with $J/\\psi\\to\\mu^+\\mu^-$ and $K^{*0}\\to K^+ \\pi^-$. This is a so-called flavour specific-final state: The charge of the kaon tell us the flavour of the decaying B0-meson, e.g. whether it was decaying as a $B^0$ or $\\bar{B}^0$.\n",
        "\n",
        "To measure the time-dependent oscillations we need three ingredients, namely:\n",
        "* the decay time\n",
        "* the flavour of the B at decay\n",
        "* the flavour of the B at production\n",
        "You have learned in the lectures that the the rate for a B produced as $B^0$ to decay as $B^0$ or $\\bar{B}^0$ is given by\n",
        "\n",
        "$ N( B^0 \\to B^0 ) = \\frac{e^{-t/\\tau}}{2} ( 1 + \\cos( \\Delta m t) ) $\n",
        "\n",
        "$ N( B^0 \\to \\bar{B}^0 ) = \\frac{e^{-t/\\tau}}{2} ( 1 - \\cos( \\Delta m t) ) $\n",
        "\n",
        "The formulas for B-mesons starting their life as an $\\bar{B}^0$ can be obtained by swapping $B^0$ and $\\bar{B}^0$.\n",
        "\n",
        "There are two important experimental effects for this measurement:\n",
        "* the sample has a non-negligible background\n",
        "* the flavour tagging has a considerable 'mistag rate'\n",
        "\n",
        "In the following we will have to deal with these two effects.\n",
        "\n",
        "## The technicalities\n",
        "\n",
        "I'm still trying to figure out how much to give away and how much to let you solve yourself. In the following I have mostly implemented the solutions for you: If you uncomment line with a single hash (#), then you can run the cells and obtain the solution. For a workbook with a full solution, you can also look at 'mixingfrequency_solution.ipynb'.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4QC0p0Bc3VH"
      },
      "source": [
        "## Prerequites\n",
        "\n",
        "Install the `zfit` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arsl6VsJfEbB"
      },
      "outputs": [],
      "source": [
        "# @ Prerequisites\n",
        "import platform\n",
        "print(platform.python_version())\n",
        "\n",
        "# In your own conda installation, use conda rather than pip\n",
        "#!conda install zfit --channel conda-forge\n",
        "#!conda install hepstats --channel conda-forge\n",
        "#!conda install mlphep --channel conda-forge\n",
        "\n",
        "# In google colab, use pip rather than conda\n",
        "!pip install zfit\n",
        "!pip install hepstats\n",
        "#!pip install mlphep\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_T7oPHje3py"
      },
      "source": [
        "## Draw the mass distribution\n",
        "\n",
        "At [this location](http://www.nikhef.nl/~wouterh/tmp/kstarntuple_for_BND.pkl.bz2) you will find a pickle file with a pandas data frame. For this exercise, the relevant fields are:\n",
        "* `mass`: the B candidate invariant mass in MeV\n",
        "* `decaytime`: the B candidate decaytime in ns\n",
        "* `q`: the charge of the B candidate reconstructed by the flavour tagging algorithm\n",
        "* `eta`: the mistagrate assigned by the flavour tagging algorithm\n",
        "* `pid`: the PDG value that the LHCb software assigned to the decaying B: This can be either 512 (for $B^0$) or -512 (for anti-$B^0$), depending on whether the kaon was $K^+$ or $K^-$.\n",
        "\n",
        "Draw the reconstructed invariant mass with your favourite tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK2PJdqUfiS_"
      },
      "outputs": [],
      "source": [
        "# The input file is accessible both as a root file and a pandas dataframe:\n",
        "# http://www.nikhef.nl/~wouterh/tmp/kstarntuple_for_BND.root\n",
        "# http://www.nikhef.nl/~wouterh/tmp/kstarntuple_for_BND.pkl.bz2\n",
        "\n",
        "# This is a partial solution using the dataframe. Earlier we used a root file\n",
        "# and uproot, but that it a little too slow on Google Colab\n",
        "\n",
        "#import pandas as pd\n",
        "#df = pd.read_pickle(\"http://www.nikhef.nl/~wouterh/tmp/kstarntuple_for_BND.pkl.bz2\")\n",
        "#mass = df['mass']\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#plt.hist(mass, bins=200)\n",
        "#plt.show()\n",
        "\n",
        "# Note: you will see the shape of the background better if you plot on a logscale!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EcQ5XaTWdDi"
      },
      "source": [
        "## Draw the decaytime distribution\n",
        "\n",
        "Draw also the B candidate 'decaytime'. The units are in nanoseconds. Compute the average decaytime and its statistical error. How does the answer compare to the average $B^0$ lifetime in the PDG? Give two reasons why the two are different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOtdtg19sNZJ"
      },
      "outputs": [],
      "source": [
        " # You can compute an average and rms with np.mean and np.std.\n",
        " # The error on the mean is sqrt(RMS/(N-1))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3KP9Vw7bUfm"
      },
      "source": [
        "## Fit the mass distribution\n",
        "\n",
        "We will now perform a fit to the invariant mass distribution to extract the number of $B^0$ events. Because it may take you too much time to figure this out yourself, we have written most of the code for you: all yyou need to do is uncomment the actual lines of python (not the comments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51V7MhV_fnfe"
      },
      "outputs": [],
      "source": [
        "# Import zfit\n",
        "\n",
        "#import zfit\n",
        "#import numpy as np\n",
        "\n",
        "# temporary hack, to make sure we can rerun this cell as often as we like.\n",
        "#from collections import OrderedDict\n",
        "#zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "# Specify the mass range. This is need for the fit.\n",
        "#massmin = 5150\n",
        "#massmax = 5350\n",
        "\n",
        "# Create a zfit data set from the dataframe. There is one thing tricky here:\n",
        "# when the entries are outside the min/max range, they are\n",
        "# ommitted when reading the dataframe. To prevent that, we make a selection\n",
        "# beforehand. We also omit the untagged events (q=0), because we do not have\n",
        "# much use for them later.\n",
        "#in_mass_range = np.logical_and(df['mass']>massmin,df['mass']<massmax)\n",
        "#tagged = df['q']!=0\n",
        "#selection = np.logical_and(in_mass_range,tagged)\n",
        "# Note that this overwrites the original data frame!\n",
        "#df = df[selection]\n",
        "#massobs = zfit.Space(\"mass\",(massmin,massmax))\n",
        "#zdata = zfit.Data.from_pandas( df, obs = massobs )\n",
        "\n",
        "# Create a zfit pdf for the B0 signal.\n",
        "#mu_B0 = zfit.Parameter(\"mu_B0\", 5279, 5250, 5300)\n",
        "#sigma_B0 = zfit.Parameter(\"sigma_B0\", 10, 0, 30)\n",
        "#masspdf_B0 = zfit.pdf.Gauss(mu=mu_B0, sigma=sigma_B0, obs=massobs)\n",
        "\n",
        "# Create a zfit pdf for the exponential background\n",
        "#lambd = zfit.Parameter(\"lambda\", -0.001, -1,+1)\n",
        "#masspdf_bkg = zfit.pdf.Exponential(lambd, obs=massobs)\n",
        "\n",
        "# create an extended PDF from the sum of these\n",
        "#nev = len( mass )\n",
        "#yield_B0  = zfit.Parameter(\"yield_B0\", 0.9*nev, -0.1*nev, 1.1*nev)\n",
        "#yield_bkg = zfit.Parameter(\"yield_bkg\", 0.1*nev, -0.1*nev, 1.1*nev)\n",
        "#extmasspdf_B0  = masspdf_B0.create_extended(yield_ = yield_B0)\n",
        "#extmasspdf_bkg = masspdf_bkg.create_extended(yield_ = yield_bkg)\n",
        "#pdf_total  = zfit.pdf.SumPDF([extmasspdf_B0, extmasspdf_bkg], name=\"totPDF\")\n",
        "\n",
        "# Create a loss function. this is what we will 'minimize'\n",
        "#nll_data = zfit.loss.ExtendedUnbinnedNLL(model=pdf_total, data=zdata)\n",
        "\n",
        "# Create the minimizer. This one uses minuit, but there are various alternatives.\n",
        "#minimizer = zfit.minimize.Minuit()\n",
        "\n",
        "# Call the minimizer to get the best fit parameters\n",
        "#result = minimizer.minimize(nll_data)\n",
        "\n",
        "# Call Hesse to get the estimated parameter errors\n",
        "#result.hesse()\n",
        "#print(result)\n",
        "\n",
        "# Draw the result\n",
        "#n_bins = 200\n",
        "#mass = df['mass']\n",
        "#plot_scaling = len(mass) / n_bins * massobs.area()\n",
        "#x = np.linspace(massmin,massmax, 1000)\n",
        "#y = pdf_total.pdf(x).numpy()\n",
        "#fig, axes = plt.subplots(2)\n",
        "#axes[1].set_yscale(\"log\")\n",
        "#for i in range(2):\n",
        "#  axis = axes[i]\n",
        "#  color = 'black'\n",
        "#  axis.hist(mass, color=color, bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
        "#  axis.hist(mass, color=color, bins=n_bins, histtype=\"step\")\n",
        "#  axis.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=2)\n",
        "#  axis.set_xlabel(\"mass [MeV]\")\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit with a better mass model\n",
        "\n",
        "If you look at the final fit result superimposed on the data set, it looks pretty bad. One reason is the 'signal mass model': it is not very well described by a Gaussian. The invariant mass distribution has 'radiative tails' due to QED corrections. Furthermore, the experimental resolution is not very entirely Gaussian.\n",
        "\n",
        "In LHCb a common solution is to fit with a more complicated model. The most popular solution is the so-called 'double-sided [Crystal Ball](https://en.wikipedia.org/wiki/Crystal_Ball_function)' function. The disadvantage of this model is that it has some highly correlated parameters, which makes the fit a little slow."
      ],
      "metadata": {
        "id": "ZrNl7HfjQlc5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMdmAGB1pPSX"
      },
      "outputs": [],
      "source": [
        "# repeat the fit but with a better mass model. This model is called a\n",
        "# 'double crystal ball'.\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "#aL = zfit.Parameter(\"aL_B0\",  1.4, 0.1, 5,floating=True)\n",
        "#aR = aL\n",
        "#nL = zfit.Parameter(\"nL_B0\", 6, 1., 10, floating=True)\n",
        "#nR = zfit.Parameter(\"nR_B0\", 10, 1., 20,floating=True)\n",
        "#masspdf_B0 = zfit.pdf.DoubleCB(obs=massobs, mu=mu_B0, sigma=sigma_B0, alphal=aL, nl=nL, alphar=aR, nr=nR)\n",
        "\n",
        "#extmasspdf_B0  = masspdf_B0.create_extended(yield_ = yield_B0)\n",
        "#extmasspdf_bkg = masspdf_bkg.create_extended(yield_ = yield_bkg)\n",
        "#pdf_total  = zfit.pdf.SumPDF([extmasspdf_B0, extmasspdf_bkg], name=\"totPDF\")\n",
        "\n",
        "#nll_data = zfit.loss.ExtendedUnbinnedNLL(model=pdf_total, data=zdata)\n",
        "# create the minimizer. This one uses minuit, but there are various alternatives.\n",
        "#result = minimizer.minimize(nll_data)\n",
        "#result.hesse()\n",
        "#print(result)\n",
        "\n",
        "# draw the result\n",
        "#n_bins = 200\n",
        "#plot_scaling = len(mass) / n_bins * massobs.area()\n",
        "#x = np.linspace(massmin,massmax, 1000)\n",
        "#y = pdf_total.pdf(x).numpy()\n",
        "#fig, axes = plt.subplots(2)\n",
        "#axes[1].set_yscale(\"log\")\n",
        "#for i in range(2):\n",
        "#  axis = axes[i]\n",
        "#  color = 'black'\n",
        "#  axis.hist(mass, color=color, bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
        "#  axis.hist(mass, color=color, bins=n_bins, histtype=\"step\")\n",
        "#  axis.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=2)\n",
        "#  axis.set_xlabel(\"mass [MeV]\")\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the s-weights for background subtraction\n",
        "\n",
        "To extract the decaytime distribution, we need to subtract the background. We could model the decaytime distribution background and perform a 2D fit to mass and decaytime, but there is an alternative solution that works without a model: Based on the mass fit we can compute so-called s-weights. (See the [sPlot paper](https://arxiv.org/abs/physics/0402083) for details.)  By weighting the events with s-weights, we effectively perform a statistically-optimal background subtraction.\n",
        "\n",
        "Compute the s-weights using the `hepstats.splot.compute_weights function`. Draw the background subtracted decay time distribution."
      ],
      "metadata": {
        "id": "vUtWa9EBYP43"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPrbQlea2QRQ"
      },
      "outputs": [],
      "source": [
        "# @ compute s-weights and make a decaytime plot\n",
        "\n",
        "# use the compute_sweights function from hepstats\n",
        "#from hepstats.splot import compute_sweights\n",
        "#sweights_all = compute_sweights(pdf_total,mass)\n",
        "#sweights_B0 = sweights_all[yield_B0]\n",
        "\n",
        "# make a background subtracted decay time plot\n",
        "#decaytime = df['decaytime']\n",
        "#plt.hist(decaytime, bins=200, label=\"all\")\n",
        "#plt.hist(decaytime, bins=200,weights = sweights_B0, label=\"signal\")\n",
        "#plt.hist(decaytime, bins=200,weights = 1-sweights_B0, label=\"background\")\n",
        "#plt.yscale(\"log\")\n",
        "#plt.legend()\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate mixed and unmixed decays\n",
        "\n",
        "We will now use the tagging information.lot the decay time distribution separately for the 'mixed' $(q*pid<0)$ and 'unmixed' $(q*pid>0)$ events."
      ],
      "metadata": {
        "id": "iIy6wg0xZ9yI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW_KeeQw6J7B"
      },
      "outputs": [],
      "source": [
        "# plot the s-weighted decay time distribution separately for 'mixed' decays\n",
        "# and for 'unmixed' decays\n",
        "#decaytime = df['decaytime']\n",
        "#q = df[\"q\"]\n",
        "#eta = df[\"eta\"]\n",
        "#pid = df[\"pid\"]\n",
        "#plt.hist(decaytime, bins=200, weights = sweights_B0 * (pid*q>0), label=\"unmixed\", alpha = 0.5)\n",
        "#plt.hist(decaytime, bins=200, weights = sweights_B0 * (pid*q<0), label=\"mixed\", alpha = 0.5)\n",
        "#plt.legend()\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw the binned asymmetry\n",
        "\n",
        "So observe the oscillation, we now plot the asymmetry in bins of decay time.\n",
        "\n",
        "To get statistical optimal results, we exploit also the estimated mistag rate, `eta`. The raw asymmetry is given by\n",
        "\n",
        "$A = \\frac{N( \\text{unmixed} ) - N(\\text{mixed})}{N( \\text{unmixed} ) + N(\\text{mixed})}$\n",
        "\n",
        "If we define a 'mixed' variable\n",
        "\n",
        "$ q = q_\\text{prod} q_\\text{decay} $\n",
        "\n",
        "we could compute the asymmetry as\n",
        "\n",
        "$ A \\; = \\; \\frac{\\sum_i w_i q_i}{\\sum w_i} $\n",
        "\n",
        "with $w_i$ the s-weight.\n",
        "\n",
        "The dilution on the asymmetry due to a constant mistagrate $\\eta$, is $D = (1-2\\eta)$, which just gives a scale factor for $A$. It turns out (not shown here), that if the dilution is not constant, the dilution-corrected asymmetry is given by\n",
        "\n",
        "$ A = \\frac{\\sum_i w_i q_i D_i}{\\sum w_i D_i^2} $\n",
        "\n",
        "where $D_i = 1-2\\eta_i$ is the per-event estimated dilution. The estimated variance in $A$ is approximately given by\n",
        "\n",
        "$\\text{var}(A) \\; = \\; \\frac{\\sum w^2_i D_i^2}{\\left(\\sum_i w_i D_i^2\\right)^2} $\n",
        "\n",
        "(Check for yourself that this formula makes sense if all weights are one and the dilution is constant.)\n",
        "\n",
        "Draw the asymmetry in bins of decay time.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "745zFbJLbW3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EWYqUvqpaN9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is a suggested binning for asymmetry plot. It is is in pico-seconds,\n",
        "# so needs to be converted to nano-seconds.\n",
        "tbins = np.array([0.002,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,5.5,6.0,6.5,7.5,8.8,10.2,11.5,15.])\n",
        "tbins = tbins/1000.\n",
        "\n",
        "# Compute the per-event value of q_prod*q_decay*(1-2eta)\n",
        "#qDecay = np.where(pid<0,-1,+1)\n",
        "#qD = q*(1-2*eta)*qDecay\n",
        "\n",
        "# Compute the sums needed to get the asymmetry and it variance.\n",
        "#wqDsum, bin_edges  = np.histogram(decaytime,bins=tbins,weights=sweights_B0*qD)\n",
        "#wqD2sum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*qD*qD)\n",
        "#w2qD2sum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*sweights_B0*qD*qD)\n",
        "#asymmetry    = wqDsum / wqD2sum\n",
        "#asymmetryerr = np.sqrt(w2qD2sum) / wqD2sum\n",
        "\n",
        "# compute in every bin the average decay time\n",
        "#wtsum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*decaytime)\n",
        "#wsum,  bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0)\n",
        "#avtime = wtsum / wsum\n",
        "\n",
        "# now draw points with error bars\n",
        "#xerrors = [avtime-bin_edges[:-1],bin_edges[1:]-avtime]\n",
        "#plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "#plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qsd-chORIp4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit the asymmetry\n",
        "\n",
        "The theoretical model for the mixing asymmetry is\n",
        "\n",
        "$A = \\cos(\\Delta m\\: t)$\n",
        "\n",
        "However, there is a small experimental problem left: The output of the tagging algorithm is not very well calibrated. Therefore, we need to take into account that the amplitude is wrong by a *dilution scale factor* which we call $f_D$. We will need $f_D$ when we measure $\\sin(2\\beta)$ tomorrow.\n",
        "\n",
        "Fit the binned asymmetry to the model\n",
        "\n",
        "$A^\\text{obs} = f_D \\cos(\\Delta m\\: t)$\n",
        "\n",
        "to extract the mixing frequency and the dilution scale factor.\n",
        "\n",
        "How well does the value of $\\Delta m$ agree with the PDG value?\n"
      ],
      "metadata": {
        "id": "avIaZMiNem0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below performs the fit to the binned asymmetry.\n",
        "# You can also implement an unbinned fit: A solution is written in the solution\n",
        "# workbook but I didn't yet get it to run on Google Colab.\n",
        "\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "# declare the parameters\n",
        "#deltaM = zfit.Parameter(\"DeltaM\",500,450,550,floating=True)\n",
        "#A0 = zfit.Parameter(\"A0\",1,0.5,1.5,floating=True)\n",
        "\n",
        "# define a function that evaluates the chi2 using the asymmetry computed above\n",
        "#def chi2( params ):\n",
        "#  A0 = params[0]\n",
        "#  dm = params[1]\n",
        "#  cosdmt = np.cos(dm * avtime)\n",
        "#  res = A0*cosdmt - asymmetry\n",
        "#  var = np.square(asymmetryerr)\n",
        "#  chi2 = np.sum( np.square(res)/var)\n",
        "#  return chi2\n",
        "\n",
        "# create a loss function and minimize it\n",
        "#loss = zfit.loss.SimpleLoss(chi2, [A0,deltaM], errordef=1)\n",
        "#result = minimizer.minimize(loss)\n",
        "#result.hesse()\n",
        "#print(result)\n",
        "\n",
        "# draw the result\n",
        "#plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "#x = np.linspace(0,0.015, 1000)\n",
        "#y = A0*np.cos(x*deltaM)\n",
        "#plt.plot(x,y)\n",
        "#plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "eTPe3Wh9SeRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform an unbinned maximum likelihood fit\n",
        "\n",
        "This doesn't quite work yet, although we managed to get it to work outside the notebook."
      ],
      "metadata": {
        "id": "04M9lkoXiGPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class BMixingPDF(zfit.pdf.BasePDF):\n",
        "    \"\"\" Generic decay PDF with mixing and CP violation\"\"\"\n",
        "    def __init__(self,obs,A0,deltaM):\n",
        "        self.A0 = A0\n",
        "        self.deltaM = deltaM\n",
        "        paramdict = { x.name : x for x in [A0,deltaM]}\n",
        "        super().__init__(obs=obs, params=paramdict, name=\"MixingPDF\")\n",
        "    def _norm(): return 1\n",
        "    def pdf(self, x):\n",
        "      #tf.print(x)\n",
        "      decaytime = x.unstack_x('decaytime')\n",
        "      qD = x.unstack_x('qD')\n",
        "      return (1 +  qD * self.A0 * tf.cos(self.deltaM*decaytime))\n",
        "\n",
        "fitobs = zfit.Space(\"decaytime\",(0.,0.015)) * zfit.Space(\"qD\",(-2,2))\n",
        "df[\"qD\"] = qD\n",
        "zdata = zfit.Data.from_pandas( df, obs=fitobs, weights=sweights_B0 )\n",
        "\n",
        "pdf  = BMixingPDF(obs=fitobs,A0=A0,deltaM=deltaM)\n",
        "nll_data = zfit.loss.UnbinnedNLL(model=pdf, data=zdata)\n",
        "minimizer = zfit.minimize.Minuit()\n",
        "result = minimizer.minimize(nll_data)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "cESfaEVzid2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}