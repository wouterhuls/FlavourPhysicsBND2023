{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3jAddM5JFDThjWK75O8jT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wouterhuls/FlavourPhysicsBND2023/blob/main/sin2beta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this exercise we are going to use an LHCb run-2 dataset to measure an angle of the CKM matrix. It is the real dataset, but you do get a little less information than the LHCb analysts have, so the statistical error is a bit larger than in the LHCb publication (https://arxiv.org/abs/2309.09728) . In particular, you will only work with one final state and one 'flavour tagger'. Furthermore, you will miss essential input from a calibration channel that is needed to calibrate the observed asymmetry: for that you can run the 'mixing' workbook. But more about that later.\n",
        "\n",
        "The learning goals of this exercise are:\n",
        "* fitting with zfit, a python based modeling package based on Tensorflow. For more instructions, visit the zfit documentation.\n",
        "* making s-plots and using s-weights for fitting\n",
        "* extracting the CKM parameter `sin(2*beta)` from a time-dependent asymmetry.\n",
        "\n",
        "The analysis will consist of two steps. In the first step you will learn about zfit and s-weights. In the second step you will measure sin(2*beta).\n",
        "\n",
        "# The physics\n",
        "\n",
        "We will discuss the physics in the lectures, but give a brief summary here. The 'golden channel' for measuring sin(2beta) is the decay $B^0 \\to J/\\psi K_s$, with $J/\\psi\\to \\mu^+ \\mu^-$ and $K_s \\to \\pi^+ \\pi^-$. The final state is a CP-conjugate state: replace every particle by anti-particle and vice versa, and you have the same final state. As a result the final state is equally accessible to a $B^0$ decay and an anti-$B^0$ decay. However, due to the interference of 'mixed' and 'unmixed' decays a difference in the decay time distributions of $B^0$ and anti-$B^0$.\n",
        "\n",
        "Ignoring $\\Delta\\Gamma_d$, the decay time distributions can be parametrized as\n",
        "\n",
        "$ N( B^0 \\to  J/\\psi K_s )    \\propto \\frac{e^{-t/\\tau}}{\\tau} \\big[ 1 + S \\sin(\\Delta m t) - C \\cos( \\Delta m t) \\big] $\n",
        "\n",
        "$ N( \\bar{B}^0 \\to J/\\psi K_s) \\propto \\frac{e^{-t/\\tau}}{\\tau} \\big[ 1 - S \\sin(\\Delta m t) + C \\cos( \\Delta m t) \\big]$\n",
        "\n",
        "In the SM, $C\\approx0$ and $S=\\sin(2\\beta)$.\n",
        "\n",
        "\n",
        "To measure the time-dependent oscillations we need three ingredients, namely:\n",
        "* the decay time\n",
        "* the flavour of the B at production\n",
        "\n",
        "There are two important experimental effects for this measurement:\n",
        "* the sample has a non-negligible background\n",
        "* the flavour tagging has a considerable 'mistag rate'\n",
        "\n",
        "In the following we will have to deal with these two effects."
      ],
      "metadata": {
        "id": "n-k9pmBVBqep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequites\n",
        "\n",
        "Install the `zfit` package."
      ],
      "metadata": {
        "id": "X4QC0p0Bc3VH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arsl6VsJfEbB"
      },
      "outputs": [],
      "source": [
        "# @ Prerequisites: install zfit. this will install uproot as well.\n",
        "!pip install zfit\n",
        "!pip install hepstats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "At this location http://www.nikhef.nl/~wouterh/tmp/ksntuple.pkl.bz2 you can find a file with a pandas dataframe with the following fields:\n",
        "* `mass`: the B candidate invariant mass in MeV\n",
        "* `decaytime`: the B candidate decaytime in ns\n",
        "* `qtag`: the charge of the B candidate reconstructed by the flavour tagging algorithm\n",
        "* `etatag`: the mistagrate assigned by the flavour tagging algorithm\n",
        "\n",
        "Load the dataset with your favourite tool and draw the reconstructed invariant mass. If you plot it on a log scale, you will find three peaks on a falling exponential background. Knowing that this is a decay to $J/\\psi K_s$ and using the PDG, identify the two peaks on the right. What is the left-most peak?"
      ],
      "metadata": {
        "id": "y_T7oPHje3py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example solution\n",
        "# This is a partial solution to the exercise using uproot. You can also use pyroot if you prefer.\n",
        "# url ='https://github.com/wouterhuls/sin2beta/raw/main/ntuple_for_BND.root'\n",
        "#url = 'http://www.nikhef.nl/~wouterh/tmp/ntuple_for_BND.root'\n",
        "#import uproot\n",
        "#events = uproot.open(url + \":tree\")\n",
        "#mass = events[\"mass\"].array()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_pickle(\"http://www.nikhef.nl/~wouterh/tmp/ksntuple.pkl.bz2\")\n",
        "mass = df['mass']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(mass, bins=200)\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "# Hint: you will see a lot more if you plot on a log scale and with more bins!"
      ],
      "metadata": {
        "id": "AK2PJdqUfiS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "Draw also the B candidate 'decaytime'. The units are in nanoseconds. Compute the average decaytime and its statistical error. How does the answer compare to the average $B^0$ lifetime in the PDG? Give two reasons why the two are different.\n",
        "\n",
        "Make a profile plot with the average decay time versus the mass.\n"
      ],
      "metadata": {
        "id": "0EcQ5XaTWdDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4\n",
        "\n",
        "We will now perform a fit to the invariant mass distribution to extract the number of $B^0$ events. Because it may take you too much time to figure this out yourself, we have written most of the code for you.\n",
        "\n",
        "If you look at the final fit result superimposed on the data set, it looks pretty bad. One reason is the 'signal mass model': it is not very well described by a Gaussian. The other reason is that the $B_s$ peak is not modeled. **Your exercise: add a third component to the mass pdf to model the $B_s$ decay.**\n",
        "\n",
        "*Hint: The shape of the $B_s$ mass peak is essentially identical to that of the $B_0$. The fit is more stable if the only 'free' parameter of the $B_s$ model is the yield. You can reuse the 'sigma' parameter of the $B_0$ pdf, but the mean is shifted. To get the shifted 'mu' for the $B_s$, use\n",
        "\n",
        "Bonus exercise:"
      ],
      "metadata": {
        "id": "i3KP9Vw7bUfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OieprbtlBmiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zfit\n",
        "import numpy as np\n",
        "\n",
        "# temporary hack, to make sure we can rerun this cell as often as we like.\n",
        "from collections import OrderedDict\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "# Specify the mass range. This is a little tricky: for now we just take all events, but the mass fit needs the right range\n",
        "massmin = 5160\n",
        "massmax = 5900\n",
        "\n",
        "# Create a zfit data set from the dataframe. There is one thing tricky here:\n",
        "# when the entries are outside the min/max range, they are\n",
        "# ommitted when reading the dataframe. To prevent that, we make a selection\n",
        "# beforehand. We also omit the untagged events (q=0), because we do not have\n",
        "# much use for them later.\n",
        "in_mass_range = np.logical_and(df['mass']>massmin,df['mass']<massmax)\n",
        "tagged = df['qtag']!=0\n",
        "selection = np.logical_and(in_mass_range,tagged)\n",
        "\n",
        "# Note that this overwrites the original data frame!\n",
        "df = df[selection]\n",
        "massobs = zfit.Space(\"mass\",(massmin,massmax))\n",
        "zdata = zfit.Data.from_pandas( df, obs = massobs )\n",
        "\n",
        "# create a zfit pdf for the B0 signal\n",
        "mu_B0 = zfit.Parameter(\"mu_B0\", 5279, 5250, 5300)\n",
        "sigma_B0 = zfit.Parameter(\"sigma_B0\", 10, 0, 30)\n",
        "masspdf_B0 = zfit.pdf.Gauss(mu=mu_B0, sigma=sigma_B0, obs=massobs)\n",
        "\n",
        "# create a zfit pdf for the exponential background\n",
        "lambd = zfit.Parameter(\"lambda\", -0.001, -1,+1)\n",
        "masspdf_bkg = zfit.pdf.Exponential(lambd, obs=massobs)\n",
        "\n",
        "# create a zfit pdf for the Bs signal. here is a trick that you need:\n",
        "sigma_Bs = sigma_B0\n",
        "#def mu_Bs_func(params): return params + 87.45 # mass shift\n",
        "#mu_Bs = zfit.ComposedParameter(\"mu_Bs\",mu_Bs_func, params=[mu_B0])\n",
        "mu_Bs = zfit.Parameter(\"mu_Bs\", 5366., 5350., 5380.)\n",
        "masspdf_Bs = zfit.pdf.Gauss(mu=mu_Bs, sigma=sigma_Bs, obs=massobs)\n",
        "\n",
        "# create an extended PDF from the sum of these\n",
        "nev = len( mass )\n",
        "yield_B0  = zfit.Parameter(\"yield_B0\", 0.9*nev, -0.1*nev, 1.1*nev)\n",
        "yield_Bs  = zfit.Parameter(\"yield_Bs\", 0.05*nev, -0.1*nev, 1.1*nev)\n",
        "yield_bkg = zfit.Parameter(\"yield_bkg\", 0.1*nev, -0.1*nev, 1.1*nev)\n",
        "extmasspdf_B0  = masspdf_B0.create_extended(yield_ = yield_B0)\n",
        "extmasspdf_Bs  = masspdf_Bs.create_extended(yield_ = yield_Bs)\n",
        "extmasspdf_bkg = masspdf_bkg.create_extended(yield_ = yield_bkg)\n",
        "pdf_total  = zfit.pdf.SumPDF([extmasspdf_B0, extmasspdf_bkg, extmasspdf_Bs], name=\"totPDF\")\n",
        "\n",
        "# create a loss function. this is what we will 'minimize'\n",
        "nll_data = zfit.loss.ExtendedUnbinnedNLL(model=pdf_total, data=zdata)\n",
        "# create the minimizer. This one uses minuit, but there are various alternatives.\n",
        "minimizer = zfit.minimize.Minuit()\n",
        "result = minimizer.minimize(nll_data)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n",
        "# draw the result\n",
        "n_bins = 200\n",
        "npdata = zdata['mass'].numpy()\n",
        "plot_scaling = len(npdata) / n_bins * massobs.area()\n",
        "x = np.linspace(massmin,massmax, 1000)\n",
        "y = pdf_total.pdf(x).numpy()\n",
        "fig, axes = plt.subplots(2)\n",
        "axes[1].set_yscale(\"log\")\n",
        "for i in range(2):\n",
        "  axis = axes[i]\n",
        "  color = 'black'\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"step\")\n",
        "  axis.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=2)\n",
        "  axis.set_xlabel(\"mass [MeV]\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "51V7MhV_fnfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit with a better mass model\n",
        "\n",
        "If you look at the final fit result superimposed on the data set, it looks pretty bad. One reason is the 'signal mass model': it is not very well described by a Gaussian. The invariant mass distribution has 'radiative tails' due to QED corrections. Furthermore, the experimental resolution is not very entirely Gaussian.\n",
        "\n",
        "In LHCb a common solution is to fit with a more complicated model. The most popular solution is the so-called 'double-sided [Crystal Ball](https://en.wikipedia.org/wiki/Crystal_Ball_function)' function. The disadvantage of this model is that it has some highly correlated parameters, which makes the fit a little slow.\n",
        "\n",
        "Note how much the signal yields change: the error due to the poor mass model is certainly not covered by the statistical error!"
      ],
      "metadata": {
        "id": "aRBOii72Who8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZGikQ3MQWhex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat the fit but with a better mass model\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "aL = zfit.Parameter(\"aL_B0\",  1.4, 0.1, 5,floating=True)\n",
        "aR = zfit.Parameter(\"aR_B0\",  1.4, 0.1, 5,floating=True)\n",
        "aR = aL\n",
        "nL = zfit.Parameter(\"nL_B0\", 6, 1., 10, floating=True)\n",
        "nR = zfit.Parameter(\"nR_B0\", 10, 1., 20,floating=True)\n",
        "\n",
        "masspdf_B0 = zfit.pdf.DoubleCB(obs=massobs, mu=mu_B0, sigma=sigma_B0, alphal=aL, nl=nL, alphar=aR, nr=nR)\n",
        "masspdf_Bs = zfit.pdf.DoubleCB(obs=massobs, mu=mu_Bs, sigma=sigma_B0, alphal=aL, nl=nL, alphar=aR, nr=nR)\n",
        "\n",
        "#masspdf_B0 = zfit.pdf.CrystalBall(obs=massobs, mu=mu_B0, sigma=sigma_B0, alpha=aL, n=nL)\n",
        "#masspdf_Bs = zfit.pdf.CrystalBall(obs=massobs, mu=mu_Bs, sigma=sigma_B0, alpha=aL, n=nL)\n",
        "\n",
        "\n",
        "extmasspdf_B0  = masspdf_B0.create_extended(yield_ = yield_B0)\n",
        "extmasspdf_Bs  = masspdf_Bs.create_extended(yield_ = yield_Bs)\n",
        "extmasspdf_bkg = masspdf_bkg.create_extended(yield_ = yield_bkg)\n",
        "pdf_total  = zfit.pdf.SumPDF([extmasspdf_B0, extmasspdf_bkg, extmasspdf_Bs], name=\"totPDF\")\n",
        "\n",
        "nll_data = zfit.loss.ExtendedUnbinnedNLL(model=pdf_total, data=zdata)\n",
        "# create the minimizer. This one uses minuit, but there are various alternatives.\n",
        "result = minimizer.minimize(nll_data)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n",
        "# draw the result\n",
        "n_bins = 200\n",
        "npdata = zdata['mass'].numpy()\n",
        "plot_scaling = len(npdata) / n_bins * massobs.area()\n",
        "x = np.linspace(massmin,massmax, 1000)\n",
        "y = pdf_total.pdf(x).numpy()\n",
        "fig, axes = plt.subplots(2)\n",
        "axes[1].set_yscale(\"log\")\n",
        "for i in range(2):\n",
        "  axis = axes[i]\n",
        "  color = 'black'\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
        "  axis.hist(npdata, color=color, bins=n_bins, histtype=\"step\")\n",
        "  axis.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=2)\n",
        "  axis.set_xlabel(\"mass [MeV]\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fMdmAGB1pPSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the s-weights for background subtraction\n",
        "\n",
        "To extract the decaytime distribution, we need to subtract the background. We could model the decaytime distribution background and perform a 2D fit to mass and decaytime, but there is an alternative solution that works without a model: Based on the mass fit we can compute so-called s-weights. (See the [sPlot paper](https://arxiv.org/abs/physics/0402083) for details.)  By weighting the events with s-weights, we effectively perform a statistically-optimal background subtraction.\n",
        "\n",
        "Compute the s-weights using the `hepstats.splot.compute_weights function`. Draw the background subtracted decay time distribution."
      ],
      "metadata": {
        "id": "vUtWa9EBYP43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @ compute s-weights and make a decaytime plot\n",
        "\n",
        "# use the compute_sweights function from hepstats\n",
        "from hepstats.splot import compute_sweights\n",
        "sweights_all = compute_sweights(pdf_total,df['mass'])\n",
        "sweights_B0 = sweights_all[yield_B0]\n",
        "\n",
        "# make a background subtracted decay time plot\n",
        "decaytime = df['decaytime']\n",
        "plt.hist(decaytime, bins=200, label=\"all\")\n",
        "plt.hist(decaytime, bins=200,weights = sweights_B0, label=\"signal\")\n",
        "plt.hist(decaytime, bins=200,weights = 1-sweights_B0, label=\"background\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JPrbQlea2QRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the s-weighted tag-weighted decay time distribution\n",
        "import matplotlib.pyplot as plt\n",
        "decaytime = df['decaytime']\n",
        "qtag = df[\"qtag\"]\n",
        "eta = df[\"etatag\"]\n",
        "plt.hist(decaytime, bins=200, weights = sweights_B0 * (qtag<0)* (1-2*eta))\n",
        "plt.hist(decaytime, bins=200, weights = sweights_B0 * (qtag>0)* (1-2*eta))\n",
        "plt.show()\n",
        "\n",
        "# suggested binning for asymmetry plot\n",
        "#tbins = np.concatenate((np.arange(0.0000,5.0,step=0.5),[5.6,6.2,7.0,7.8,8.7,9.7,10.8,12.0,13.4,15.]))\n",
        "tbins = np.array([0.002,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.2,6.2,7.5,8.8,10.2,11.5,15.])\n",
        "tbins = tbins/1000.\n",
        "\n",
        "# choose 20 bins with equal number of events\n",
        "#tbins = np.quantile(decaytime, np.linspace(start=0.0,stop=1.0,num=21)[1:])\n",
        "#tbins[-1] = 0.015\n",
        "#tbins[0]  = 0.0002\n",
        "\n",
        "# The minus one has something to do with the meaning of the tag\n",
        "qD = qtag*(1-2*eta)*-1\n",
        "wqDsum, bin_edges  = np.histogram(decaytime,bins=tbins,weights=sweights_B0*qD)\n",
        "wqD2sum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*qD*qD)\n",
        "w2qD2sum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*sweights_B0*qD*qD)\n",
        "asymmetry    = wqDsum / wqD2sum\n",
        "asymmetryerr = np.sqrt(w2qD2sum) / wqD2sum\n",
        "\n",
        "# compute in every bin the average decay time\n",
        "wtsum, bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0*decaytime)\n",
        "wsum,  bin_edges = np.histogram(decaytime,bins=tbins,weights=sweights_B0)\n",
        "avtime = wtsum / wsum\n",
        "\n",
        "# now draw points with both vertical and horizontal errors\n",
        "xerrors = [avtime-bin_edges[:-1],bin_edges[1:]-avtime]\n",
        "plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "eW_KeeQw6J7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "60ZV6zLJi-8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the asymmetry\n",
        "\n",
        "The theoretical model for the mixing asymmetry is\n",
        "\n",
        "$A_{CP} = S \\sin(\\Delta m\\: t) - C \\cos(\\Delta m\\:t ) $\n",
        "\n",
        "Fit this model to the data taking into account the dilution.\n",
        "\n",
        "Comparing your final result to that in the paper, you will notice that it is quite far off. The reason is that the flavour tag dilution is not well calibrated. The calibration factor can be obtained from a control channel. In the 'mixing' workbook the $B_d \\to J/\\psi K^{*0}$ control channel is used to extract the dilution scale factor.\n"
      ],
      "metadata": {
        "id": "M6QPisK9i_pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below performs the fit to the binned asymmetry.\n",
        "\n",
        "from tensorflow.python.dlpack.dlpack import from_dlpack\n",
        "zfit.core.parameter.ZfitParameterMixin._existing_params = OrderedDict()\n",
        "\n",
        "# declare the parameters. You can also float deltaM if you like.\n",
        "deltaM = zfit.Parameter(\"DeltaM\",507.,450,550,floating=False)\n",
        "S = zfit.Parameter(\"S\",0.5,-1.5,1.5,floating=True)\n",
        "C = zfit.Parameter(\"C\",0.,-1.5,1.5,floating=True)\n",
        "# define a function that evaluates the chi2 using the asymmetry computed above\n",
        "def chi2( params ):\n",
        "  S = params[0]\n",
        "  C  = params[1]\n",
        "  dm = params[2]\n",
        "  cosdmt = np.cos(dm * avtime)\n",
        "  sindmt = np.sin(dm * avtime)\n",
        "  res = (S*sindmt - C*cosdmt) - asymmetry\n",
        "  var = np.square(asymmetryerr)\n",
        "  chi2 = np.sum( np.square(res)/var)\n",
        "  return chi2\n",
        "# create a loss function and minimize it\n",
        "loss = zfit.loss.SimpleLoss(chi2, [S,C,deltaM], errordef=1)\n",
        "result = minimizer.minimize(loss)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n",
        "# draw the result\n",
        "plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "x = np.linspace(0,0.015, 1000)\n",
        "y = S*np.sin(x*deltaM)-C*np.cos(x*deltaM)\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "eTPe3Wh9SeRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform an unbinned maximum likelihood fit\n",
        "\n",
        "Build a PDF and fit it to the unbinned dataset. How do the errors compare to the binned fit?\n"
      ],
      "metadata": {
        "id": "04M9lkoXiGPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this solution works but we'll once need to figure out how to do the normalization properly.\n",
        "import tensorflow as tf\n",
        "class BMixingPDF(zfit.pdf.BasePDF):\n",
        "    \"\"\" Generic decay PDF with mixing and CP violation\"\"\"\n",
        "    def __init__(self,obs,S,C,deltaM):\n",
        "        self.S = S\n",
        "        self.C = C\n",
        "        self.deltaM = deltaM\n",
        "        paramdict = { x.name : x for x in [S,C,deltaM]}\n",
        "        super().__init__(obs=obs, params=paramdict, name=\"CPDecayPDF\")\n",
        "    def _norm(): return 1\n",
        "    def _unnormalized_pdf(self, x):\n",
        "      #tf.print(x)\n",
        "      decaytime, qD = zfit.z.unstack_x(x)\n",
        "      return (1 +  qD * (S * tf.sin(self.deltaM*decaytime) - C *tf.cos(self.deltaM*decaytime)))\n",
        "    def pdf(self,x,norm=False): return self._unnormalized_pdf(x)\n",
        "\n",
        "fitobs = zfit.Space(\"decaytime\",(0.,0.015)) * zfit.Space(\"qD\",(-2,2))\n",
        "if \"qD\" not in df: df[\"qD\"] = qD\n",
        "zdata = zfit.Data.from_pandas( df, obs=fitobs, weights=sweights_B0 )\n",
        "\n",
        "pdf  = BMixingPDF(obs=fitobs,S=S,C=C,deltaM=deltaM)\n",
        "nll_data = zfit.loss.UnbinnedNLL(model=pdf, data=zdata)\n",
        "minimizer = zfit.minimize.Minuit()\n",
        "result = minimizer.minimize(nll_data)\n",
        "result.hesse()\n",
        "print(result)\n",
        "\n",
        "# draw the result\n",
        "plt.errorbar(x=avtime, y=asymmetry, xerr=xerrors, yerr=asymmetryerr,fmt='o')\n",
        "x = np.linspace(0,0.015, 1000)\n",
        "y = S*np.sin(x*deltaM)-C*np.cos(x*deltaM)\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cESfaEVzid2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}